{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "NLP_Vector_Representation_of_Text_Using.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Representation Vecteur de l'utilisation du Texte LARHCHIM ZAKARIA**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QYGPbDRlJujZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "DATA = [        \r\n",
        "\"\"\"Perhaps one of the most significant advances made by Arabic mathematics began at this time with the work of al-Khwarizmi, namely\r\n",
        "the beginnings of algebra. It is important to understand just how significant this new idea was. It was a revolutionary move away from\r\n",
        "the Greek concept of mathematics which was essentially geometry. Algebra was a unifying theory which allowed rational\r\n",
        "numbers, irrational numbers, geometrical magnitudes, etc., to all be treated as \"algebraic objects\". It gave mathematics a whole new\r\n",
        "development path so much broader in concept to that which had existed before, and provided a vehicle for future development of the\r\n",
        "subject. Another important aspect of the introduction of algebraic ideas was that it allowed mathematics to be applied to itself in a\r\n",
        "way which had not happened before.\"\"\",\r\n",
        "\r\n",
        " \"\"\"ربما كانت أحد أهم التطورات التي قامت بها الرياضيات العربية التي بدأت في هذا الوقت بعمل الخوارزمي  وهي بدايات الجبر،ومن المهم فهم كيف كانت هذه الفكرة الجديدة مهمة، فقد كانت خطوة ثورية بعيدا عن\r\n",
        "المفهوم اليوناني للرياضيات التي هي في جوهرها  هندسة، الجبركان نظرية موحدة تتحيح الأعداد الكسرية و الأعداد اللا كسرية ، والمقادير الهندسية و غيرها ، أن تتعامل على أنها أجسام جبرية، و أعطت الرياضيات ككل مسارا جديدًا للتطوربمفهوم \r\n",
        " أوسع بكثير من الذي كان موجودًا من قبل ، وقدم وسيلة للتنمية في هذا الموضوع مستقبلا .و جانب آخر مهم لإدخال أفكار الجبر و هو أنه سمح بتطبيق الرياضيات على نفسها \r\n",
        "بطريقة  لم تحدث من قبل.\"\"\"\r\n",
        "]\r\n",
        "\r\n",
        "print(DATA)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Perhaps one of the most significant advances made by Arabic mathematics began at this time with the work of al-Khwarizmi, namely\\nthe beginnings of algebra. It is important to understand just how significant this new idea was. It was a revolutionary move away from\\nthe Greek concept of mathematics which was essentially geometry. Algebra was a unifying theory which allowed rational\\nnumbers, irrational numbers, geometrical magnitudes, etc., to all be treated as \"algebraic objects\". It gave mathematics a whole new\\ndevelopment path so much broader in concept to that which had existed before, and provided a vehicle for future development of the\\nsubject. Another important aspect of the introduction of algebraic ideas was that it allowed mathematics to be applied to itself in a\\nway which had not happened before.', 'ربما كانت أحد أهم التطورات التي قامت بها الرياضيات العربية التي بدأت في هذا الوقت بعمل الخوارزمي  وهي بدايات الجبر،ومن المهم فهم كيف كانت هذه الفكرة الجديدة مهمة، فقد كانت خطوة ثورية بعيدا عن\\nالمفهوم اليوناني للرياضيات التي هي في جوهرها  هندسة، الجبركان نظرية موحدة تتحيح الأعداد الكسرية و الأعداد اللا كسرية ، والمقادير الهندسية و غيرها ، أن تتعامل على أنها أجسام جبرية، و أعطت الرياضيات ككل مسارا جديدًا للتطوربمفهوم \\n أوسع بكثير من الذي كان موجودًا من قبل ، وقدم وسيلة للتنمية في هذا الموضوع مستقبلا .و جانب آخر مهم لإدخال أفكار الجبر و هو أنه سمح بتطبيق الرياضيات على نفسها \\nبطريقة  لم تحدث من قبل.']\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K88sBRSbJanW",
        "outputId": "c3ac450f-6e7a-4c6c-e97e-9761b1974ddd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "pip install gensim"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8YZl9HXD9pO",
        "outputId": "90758955-8ae8-4b82-a490-05686fae228d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "u8VRmDbyyc-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**traitement de données**"
      ],
      "metadata": {
        "id": "5zaXy70FKK1n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "import string\r\n",
        "import nltk\r\n",
        "import re\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "nltk.download('punkt')\r\n",
        "ponctuation = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation  \r\n",
        "\r\n",
        "def Punctuation(sentence):\r\n",
        "  sentence = sentence.lower()\r\n",
        "  cleanSentence = ''.join([item for item in sentence if item not in ponctuation ])\r\n",
        "  return cleanSentence\r\n",
        "  \r\n",
        "def sentenceTokenize(text):\r\n",
        "  return word_tokenize(text)\r\n",
        "  \r\n",
        "TOKENLIST = sentenceTokenize(Punctuation(DATA[0]))\r\n",
        "print(TOKENLIST)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['perhaps', 'one', 'of', 'the', 'most', 'significant', 'advances', 'made', 'by', 'arabic', 'mathematics', 'began', 'at', 'this', 'time', 'with', 'the', 'work', 'of', 'alkhwarizmi', 'namely', 'the', 'beginnings', 'of', 'algebra', 'it', 'is', 'important', 'to', 'understand', 'just', 'how', 'significant', 'this', 'new', 'idea', 'was', 'it', 'was', 'a', 'revolutionary', 'move', 'away', 'from', 'the', 'greek', 'concept', 'of', 'mathematics', 'which', 'was', 'essentially', 'geometry', 'algebra', 'was', 'a', 'unifying', 'theory', 'which', 'allowed', 'rational', 'numbers', 'irrational', 'numbers', 'geometrical', 'magnitudes', 'etc', 'to', 'all', 'be', 'treated', 'as', 'algebraic', 'objects', 'it', 'gave', 'mathematics', 'a', 'whole', 'new', 'development', 'path', 'so', 'much', 'broader', 'in', 'concept', 'to', 'that', 'which', 'had', 'existed', 'before', 'and', 'provided', 'a', 'vehicle', 'for', 'future', 'development', 'of', 'the', 'subject', 'another', 'important', 'aspect', 'of', 'the', 'introduction', 'of', 'algebraic', 'ideas', 'was', 'that', 'it', 'allowed', 'mathematics', 'to', 'be', 'applied', 'to', 'itself', 'in', 'a', 'way', 'which', 'had', 'not', 'happened', 'before']\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0D6VE11KN_I",
        "outputId": "c0fce880-2182-4f44-e5d3-412224cbdc11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Technique de Model group Word2vec**\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Le principal problème avec BOW & TF-IDF\r\n",
        "-Il ne capture pas la position dans le texte, la sémantique, les cooccurrences dans différents documents.\r\n",
        "-TF-IDF donne de l'importance au mot non commun\r\n",
        "-Très fort risque de surapprentissage\r\n",
        "\r\n",
        "Solution :\r\n",
        "\r\n",
        "\r\n",
        "Word2Vec :\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Chaque mot est essentiellement représenté comme un vecteur de 32 dimensions ou plus au lieu d'un seul nombre\r\n",
        "L'information sémentique et la relation entre les différents mots sont également conservées\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "**Étapes pour créer Word2vec**\r\n",
        "\r\n",
        "\r\n",
        "- Tokenisation des phrases\r\n",
        "- Créer des histogrammes\r\n",
        "- Prendre les mots les plus fréquents\r\n",
        "- Créez un metrix avec tous les mots uniques. il représente également la relation d'occurrence entre les mots"
      ],
      "metadata": {
        "id": "nJqZMxVbK5dH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "from gensim.models import Word2Vec,keyedvectors"
      ],
      "outputs": [],
      "metadata": {
        "id": "23ypJgjVK-9P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "model = Word2Vec([TOKENLIST] , min_count=1, size=32)\r\n",
        "model.most_similar(\"important\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('another', 0.4236752986907959),\n",
              " ('the', 0.4023265242576599),\n",
              " ('that', 0.37143370509147644),\n",
              " ('concept', 0.34806084632873535),\n",
              " ('in', 0.21934251487255096),\n",
              " ('existed', 0.2181435525417328),\n",
              " ('arabic', 0.2060082107782364),\n",
              " ('essentially', 0.20154961943626404),\n",
              " ('began', 0.18891017138957977),\n",
              " ('understand', 0.18592569231987)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylUAQhwELEU1",
        "outputId": "d0b0b81c-aa8e-4fb7-f1d7-be56944577b7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "model.similarity(\"important\", \"mathematics\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12940413"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiOcBmFFLZHC",
        "outputId": "6f7b7610-3741-49cf-98b7-f655ff1a0a0f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "model.similar_by_word(\"one\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('and', 0.4798958897590637),\n",
              " ('greek', 0.4782176613807678),\n",
              " ('geometrical', 0.39303091168403625),\n",
              " ('had', 0.36544114351272583),\n",
              " ('to', 0.3582589030265808),\n",
              " ('a', 0.33519819378852844),\n",
              " ('at', 0.3213859796524048),\n",
              " ('perhaps', 0.2731481194496155),\n",
              " ('allowed', 0.26664671301841736),\n",
              " ('magnitudes', 0.2550728917121887)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0smNMhnLlqx",
        "outputId": "1d267c45-e0cc-4d36-efa8-d663b38e8fe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CountVectorizer**"
      ],
      "metadata": {
        "id": "GreBbmmqL1PR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "import pandas as pd\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "  \r\n",
        " \r\n",
        "CompteurVecteur = CountVectorizer(ngram_range=(1,1),stop_words='english')\r\n",
        "CompteurDATA = CompteurVecteur.fit_transform([DATA[0]])\r\n",
        "print(CompteurVecteur.get_feature_names())\r\n",
        "my_df=pd.DataFrame(CompteurDATA.toarray(),columns=CompteurVecteur.get_feature_names()).T\r\n",
        "my_df.head(10)\r\n",
        "\r\n",
        "[ ]\r\n",
        "unt_vec=CountVectorizer(token_pattern=r\"(?u)\\b\\w\\w+\\b\")\r\n",
        "arr = CompteurVecteur.fit_transform([DATA[1]]).toarray()\r\n",
        "print(arr)\r\n",
        "print('\\nvocabulary list:\\n')\r\n",
        "for key,value in CompteurVecteur.vocabulary_.items():\r\n",
        "    print(key,value)\r\n",
        "\r\n",
        "print(CompteurDATA)\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['advances', 'al', 'algebra', 'algebraic', 'allowed', 'applied', 'arabic', 'aspect', 'away', 'began', 'beginnings', 'broader', 'concept', 'development', 'essentially', 'existed', 'future', 'gave', 'geometrical', 'geometry', 'greek', 'happened', 'idea', 'ideas', 'important', 'introduction', 'irrational', 'just', 'khwarizmi', 'magnitudes', 'mathematics', 'new', 'numbers', 'objects', 'path', 'provided', 'rational', 'revolutionary', 'significant', 'subject', 'theory', 'time', 'treated', 'understand', 'unifying', 'vehicle', 'way', 'work']\n",
            "[[1 1 1 1 1 1 1 1 1 1 2 1 3 2 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 3 1 2 1 3 1 1 1 1 1 1 1 1 1 1 3 1 1 1\n",
            "  1 1 1 2 1 1 1 1 1 1 1 1 1]]\n",
            "\n",
            "vocabulary list:\n",
            "\n",
            "ربما 46\n",
            "كانت 57\n",
            "أحد 2\n",
            "أهم 8\n",
            "التطورات 11\n",
            "التي 12\n",
            "قامت 54\n",
            "بها 36\n",
            "الرياضيات 18\n",
            "العربية 19\n",
            "بدأت 30\n",
            "في 53\n",
            "هذا 75\n",
            "الوقت 27\n",
            "بعمل 33\n",
            "الخوارزمي 16\n",
            "وهي 84\n",
            "بدايات 31\n",
            "الجبر 13\n",
            "ومن 83\n",
            "المهم 24\n",
            "فهم 52\n",
            "كيف 60\n",
            "هذه 76\n",
            "الفكرة 20\n",
            "الجديدة 15\n",
            "مهمة 70\n",
            "فقد 51\n",
            "خطوة 45\n",
            "ثورية 40\n",
            "بعيدا 34\n",
            "عن 49\n",
            "المفهوم 23\n",
            "اليوناني 28\n",
            "للرياضيات 64\n",
            "هي 79\n",
            "جوهرها 44\n",
            "هندسة 77\n",
            "الجبركان 14\n",
            "نظرية 73\n",
            "موحدة 72\n",
            "تتحيح 37\n",
            "الأعداد 10\n",
            "الكسرية 21\n",
            "اللا 22\n",
            "كسرية 58\n",
            "والمقادير 80\n",
            "الهندسية 26\n",
            "غيرها 50\n",
            "أن 5\n",
            "تتعامل 38\n",
            "على 48\n",
            "أنها 7\n",
            "أجسام 1\n",
            "جبرية 42\n",
            "أعطت 3\n",
            "ككل 59\n",
            "مسارا 66\n",
            "جديد 43\n",
            "للتطوربمفهوم 62\n",
            "أوسع 9\n",
            "بكثير 35\n",
            "من 68\n",
            "الذي 17\n",
            "كان 56\n",
            "موجود 71\n",
            "قبل 55\n",
            "وقدم 82\n",
            "وسيلة 81\n",
            "للتنمية 63\n",
            "الموضوع 25\n",
            "مستقبلا 67\n",
            "جانب 41\n",
            "آخر 0\n",
            "مهم 69\n",
            "لإدخال 61\n",
            "أفكار 4\n",
            "هو 78\n",
            "أنه 6\n",
            "سمح 47\n",
            "بتطبيق 29\n",
            "نفسها 74\n",
            "بطريقة 32\n",
            "لم 65\n",
            "تحدث 39\n",
            "  (0, 38)\t2\n",
            "  (0, 0)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 30)\t4\n",
            "  (0, 9)\t1\n",
            "  (0, 41)\t1\n",
            "  (0, 47)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 28)\t1\n",
            "  (0, 10)\t1\n",
            "  (0, 2)\t2\n",
            "  (0, 24)\t2\n",
            "  (0, 43)\t1\n",
            "  (0, 27)\t1\n",
            "  (0, 31)\t2\n",
            "  (0, 22)\t1\n",
            "  (0, 37)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 20)\t1\n",
            "  (0, 12)\t2\n",
            "  (0, 14)\t1\n",
            "  (0, 19)\t1\n",
            "  (0, 44)\t1\n",
            "  (0, 40)\t1\n",
            "  (0, 4)\t2\n",
            "  (0, 36)\t1\n",
            "  (0, 32)\t2\n",
            "  (0, 26)\t1\n",
            "  (0, 18)\t1\n",
            "  (0, 29)\t1\n",
            "  (0, 42)\t1\n",
            "  (0, 3)\t2\n",
            "  (0, 33)\t1\n",
            "  (0, 17)\t1\n",
            "  (0, 13)\t2\n",
            "  (0, 34)\t1\n",
            "  (0, 11)\t1\n",
            "  (0, 15)\t1\n",
            "  (0, 35)\t1\n",
            "  (0, 45)\t1\n",
            "  (0, 16)\t1\n",
            "  (0, 39)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 25)\t1\n",
            "  (0, 23)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 46)\t1\n",
            "  (0, 21)\t1\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eRMzW66MDHB",
        "outputId": "3b8e88fe-1ae4-4332-ee38-defb03e8a92d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tfidf Vectorizer**\r\n",
        "**TF-IDF**\r\n",
        "\r\n",
        "\r\n",
        "Un problème avec la notation de la fréquence des mots est que les mots très fréquents commencent à dominer dans le document, mais peuvent ne pas contenir autant de contenu informatif pour le modèle que des mots plus rares mais peut-être spécifiques à un domaine.\r\n",
        "\r\n",
        "Une approche consiste à redimensionner la fréquence des mots en fonction de leur fréquence d'apparition dans tous les documents, de sorte que les scores des mots fréquents comme « le » qui sont également fréquents dans tous les documents soient pénalisés.\r\n",
        "\r\n",
        "Cette approche de la notation est appelée Term Frequency – Inverse Document Frequency, ou TF-IDF en abrégé, où :\r\n",
        "\r\n",
        "Fréquence du terme : est une notation de la fréquence du mot dans le document actuel.\r\n",
        "Inverse Document Frequency : est une évaluation de la rareté du mot dans les documents.\r\n",
        "Les scores sont une pondération où tous les mots ne sont pas aussi importants ou intéressants.\r\n",
        "Sans entrer dans les mathématiques, les TF-IDF sont des scores de fréquence de mots qui tentent de mettre en évidence les mots les plus intéressants, par ex. fréquentes dans un document mais pas entre les documents.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "**Avantages :**\r\n",
        "\r\n",
        "\r\n",
        "Facile à calculer\r\n",
        "Vous disposez d'une métrique de base pour extraire les termes les plus descriptifs d'un document\r\n",
        "Vous pouvez facilement calculer la similitude entre 2 documents en l'utilisant\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "**Désavantages:**\r\n",
        "\r\n",
        "\r\n",
        "TF-IDF est basé sur le modèle du sac de mots (BoW), il ne capture donc pas la position dans le texte, la sémantique, les cooccurrences dans différents documents, etc.\r\n",
        "Pour cette raison, TF-IDF n'est utile qu'en tant que fonctionnalité de niveau lexical\r\n",
        "Impossible de capturer la sémantique (par exemple, par rapport aux modèles de sujet, aux imbrications de mots)"
      ],
      "metadata": {
        "id": "-owQx9YJOe7n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "import pandas as pd\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
        "\r\n",
        " \r\n",
        "IDFVECTOR = TfidfVectorizer(use_idf=True, \r\n",
        "                        smooth_idf=False,  \r\n",
        "                        ngram_range=(1,1),stop_words='english')\r\n",
        "IDFDATA = IDFVECTOR.fit_transform([DATA[0]])\r\n",
        " \r\n",
        "IDFDATAframe=pd.DataFrame(IDFDATA.toarray(),columns=IDFVECTOR.get_feature_names()).T\r\n",
        "IDFDATAframe.head(10)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  0\n",
              "advances   0.105409\n",
              "al         0.105409\n",
              "algebra    0.210819\n",
              "algebraic  0.210819\n",
              "allowed    0.210819\n",
              "applied    0.105409\n",
              "arabic     0.105409\n",
              "aspect     0.105409\n",
              "away       0.105409\n",
              "began      0.105409"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>advances</th>\n",
              "      <td>0.105409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>al</th>\n",
              "      <td>0.105409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>algebra</th>\n",
              "      <td>0.210819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>algebraic</th>\n",
              "      <td>0.210819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>allowed</th>\n",
              "      <td>0.210819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>applied</th>\n",
              "      <td>0.105409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>arabic</th>\n",
              "      <td>0.105409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>aspect</th>\n",
              "      <td>0.105409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>away</th>\n",
              "      <td>0.105409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>began</th>\n",
              "      <td>0.105409</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "TP1etl8lOghQ",
        "outputId": "d4d67de4-e954-4e7b-d878-075f4a451b40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Bag of Words**\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Bow (Bag of Words)\r\n",
        "\r\n",
        "\r\n",
        "Le principal problème avec le texte\r\n",
        "Les algorithmes d'apprentissage automatique ne peuvent pas fonctionner directement avec du texte brut ; le texte doit être converti en nombres. Plus précisément, les vecteurs de nombres.\r\n",
        "\r\n",
        "C'est ce qu'on appelle l'extraction de caractéristiques ou l'encodage de caractéristiques.\r\n",
        "\r\n",
        "Une méthode populaire et simple d'extraction de caractéristiques avec des données textuelles s'appelle le modèle de sac de mots de texte.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Qu'est-ce qu'un Bow (Bag of Words) ?\r\n",
        "\r\n",
        "\r\n",
        "le Bow (Bag of Words) est une représentation de texte qui décrit l'occurrence de mots dans un document. Cela implique deux choses :\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "1 - Un vocabulaire de mots connus.\r\n",
        "2 - Une mesure de la présence de mots connus.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Limitations du Bow (Bag of Words)\r\n",
        "\r\n",
        "Le modèle du Bow (Bag of Words) est très simple à comprendre et à mettre en œuvre et offre une grande flexibilité pour la personnalisation de vos données textuelles spécifiques.\r\n",
        "\r\n",
        "Néanmoins, il souffre de quelques insuffisances, telles que :\r\n",
        "\r\n",
        "1- Vocabulaire : Le vocabulaire nécessite une conception soignée, notamment pour gérer la taille, ce qui impacte la rareté des représentations du document.\r\n",
        "\r\n",
        "2- La parcimonie : les représentations éparses sont plus difficiles à modéliser à la fois pour des raisons de calcul (complexité spatiale et temporelle) et également pour des raisons d'information, où le défi est pour les modèles d'exploiter si peu d'informations dans un espace de représentation aussi grand.\r\n",
        "\r\n",
        "3- Signification : Ignorer l'ordre des mots ignore le contexte, et à son tour la signification des mots dans le document (sémantique). Le contexte et la signification peuvent offrir beaucoup au modèle, que si modélisé pourrait faire la différence entre les mêmes mots disposés différemment (« c'est intéressant » vs « est-ce intéressant »), synonymes (« vieux vélo » vs « vélo d'occasion ») , et beaucoup plus."
      ],
      "metadata": {
        "id": "_e3CMaJ9Or5g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "from keras.preprocessing.text import Tokenizer\r\n",
        "\r\n",
        "model = Tokenizer() \r\n",
        "model.fit_on_texts(DATA[0])\r\n",
        "print(f'Key : {list(model.word_index.keys())}')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "print(model.word_counts)\r\n",
        "print(model.document_count)\r\n",
        "print(model.word_index)\r\n",
        "print(model.word_docs)\r\n",
        "\r\n",
        "\r\n",
        "text_to_vector = model.texts_to_matrix(DATA[0], mode='count')\r\n",
        "my_cbow_result = pd.DataFrame(text_to_vector)\r\n",
        "my_cbow_result\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key : ['e', 'a', 't', 'i', 'o', 'n', 'h', 'r', 's', 'c', 'm', 'l', 'd', 'w', 'b', 'f', 'p', 'g', 'u', 'y', 'v', 'k', 'j', 'z', 'x']\n",
            "OrderedDict([('p', 15), ('e', 77), ('r', 33), ('h', 35), ('a', 72), ('s', 33), ('o', 45), ('n', 38), ('f', 16), ('t', 67), ('m', 25), ('i', 52), ('g', 15), ('c', 26), ('d', 22), ('v', 8), ('b', 17), ('y', 9), ('w', 20), ('k', 3), ('l', 24), ('z', 1), ('u', 12), ('j', 3), ('x', 1)])\n",
            "814\n",
            "{'e': 1, 'a': 2, 't': 3, 'i': 4, 'o': 5, 'n': 6, 'h': 7, 'r': 8, 's': 9, 'c': 10, 'm': 11, 'l': 12, 'd': 13, 'w': 14, 'b': 15, 'f': 16, 'p': 17, 'g': 18, 'u': 19, 'y': 20, 'v': 21, 'k': 22, 'j': 23, 'z': 24, 'x': 25}\n",
            "defaultdict(<class 'int'>, {'p': 15, 'e': 77, 'r': 33, 'h': 35, 'a': 72, 's': 33, 'o': 45, 'n': 38, 'f': 16, 't': 67, 'm': 25, 'i': 52, 'g': 15, 'c': 26, 'd': 22, 'v': 8, 'b': 17, 'y': 9, 'w': 20, 'k': 3, 'l': 24, 'z': 1, 'u': 12, 'j': 3, 'x': 1})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      0    1    2    3    4    5    6   ...   19   20   21   22   23   24   25\n",
              "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "1    0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "4    0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
              "809  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "810  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "811  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "812  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "813  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[814 rows x 26 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>809</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>810</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>811</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>812</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>813</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>814 rows × 26 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "Rp7eE9PKOvMn",
        "outputId": "9f299669-a51f-474e-d3ba-8145b30c2181"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**sklearn.preprocessing.OneHotEncoder**"
      ],
      "metadata": {
        "id": "7Z6Y1RTzO7rA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "\r\n",
        "import numpy as np\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "\r\n",
        "values = np.array(TOKENLIST)\r\n",
        "print(values)\r\n",
        "label_encoder = LabelEncoder()\r\n",
        "my_int_encoded = label_encoder.fit_transform(values)\r\n",
        "print(my_int_encoded)\r\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\r\n",
        "my_int_encoded = my_int_encoded.reshape(len(my_int_encoded), 1)\r\n",
        "onehot_encoded_values = onehot_encoder.fit_transform(my_int_encoded)\r\n",
        "print(onehot_encoded_values)\r\n",
        "inverted_values = label_encoder.inverse_transform([np.argmax(onehot_encoded_values[0, :])])\r\n",
        "print(inverted_values)\r\n",
        "\r\n",
        "from keras.preprocessing.text import one_hot\r\n",
        "from keras.preprocessing.text import text_to_word_sequence\r\n",
        "words = set(text_to_word_sequence(DATA[0]))\r\n",
        "vocab_size = len(words)\r\n",
        "print(vocab_size)\r\n",
        "result = one_hot(DATA[0], round(vocab_size))\r\n",
        "print(result)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['perhaps' 'one' 'of' 'the' 'most' 'significant' 'advances' 'made' 'by'\n",
            " 'arabic' 'mathematics' 'began' 'at' 'this' 'time' 'with' 'the' 'work'\n",
            " 'of' 'alkhwarizmi' 'namely' 'the' 'beginnings' 'of' 'algebra' 'it' 'is'\n",
            " 'important' 'to' 'understand' 'just' 'how' 'significant' 'this' 'new'\n",
            " 'idea' 'was' 'it' 'was' 'a' 'revolutionary' 'move' 'away' 'from' 'the'\n",
            " 'greek' 'concept' 'of' 'mathematics' 'which' 'was' 'essentially'\n",
            " 'geometry' 'algebra' 'was' 'a' 'unifying' 'theory' 'which' 'allowed'\n",
            " 'rational' 'numbers' 'irrational' 'numbers' 'geometrical' 'magnitudes'\n",
            " 'etc' 'to' 'all' 'be' 'treated' 'as' 'algebraic' 'objects' 'it' 'gave'\n",
            " 'mathematics' 'a' 'whole' 'new' 'development' 'path' 'so' 'much'\n",
            " 'broader' 'in' 'concept' 'to' 'that' 'which' 'had' 'existed' 'before'\n",
            " 'and' 'provided' 'a' 'vehicle' 'for' 'future' 'development' 'of' 'the'\n",
            " 'subject' 'another' 'important' 'aspect' 'of' 'the' 'introduction' 'of'\n",
            " 'algebraic' 'ideas' 'was' 'that' 'it' 'allowed' 'mathematics' 'to' 'be'\n",
            " 'applied' 'to' 'itself' 'in' 'a' 'way' 'which' 'had' 'not' 'happened'\n",
            " 'before']\n",
            "[60 58 57 68 49 64  1 46 20 10 48 17 13 70 71 81 68 82 57  4 52 68 18 57\n",
            "  2 43 42 38 72 74 45 35 64 70 53 36 77 43 77  0 63 50 14 27 68 32 21 57\n",
            " 48 79 77 23 31  2 77  0 75 69 79  6 62 55 41 55 30 47 24 72  5 15 73 11\n",
            "  3 56 43 29 48  0 80 53 22 59 65 51 19 39 21 72 67 79 33 25 16  7 61  0\n",
            " 76 26 28 22 57 68 66  8 38 12 57 68 40 57  3 37 77 67 43  6 48 72 15  9\n",
            " 72 44 39  0 78 79 33 54 34 16]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "['perhaps']\n",
            "84\n",
            "[73, 49, 30, 37, 56, 74, 72, 22, 32, 61, 21, 33, 83, 34, 2, 77, 37, 54, 30, 4, 63, 73, 37, 53, 30, 48, 55, 25, 32, 65, 3, 34, 32, 74, 34, 22, 43, 18, 55, 18, 56, 53, 50, 19, 46, 37, 47, 73, 30, 21, 18, 18, 66, 29, 48, 18, 56, 37, 30, 18, 64, 78, 29, 33, 29, 9, 43, 7, 65, 6, 74, 9, 76, 78, 30, 55, 39, 21, 56, 62, 22, 50, 57, 45, 20, 31, 74, 73, 65, 39, 18, 45, 15, 19, 7, 21, 56, 61, 83, 78, 50, 30, 37, 32, 36, 32, 79, 30, 37, 2, 30, 78, 74, 18, 39, 55, 64, 21, 65, 74, 27, 65, 17, 74, 56, 80, 18, 45, 67, 47, 19]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1GADkRXO-P5",
        "outputId": "131d0a06-252d-48a4-9291-f9cdcbc6545e"
      }
    }
  ]
}
